# AI Memory Layer - Project Overview

## ğŸ¯ What This Project Does

A backend service that stores and retrieves conversation memories for AI chatbots. It:
- Stores messages with embeddings (text â†’ numbers)
- Finds relevant past messages when searching
- Automatically archives old/unimportant messages
- Works with any AI system via REST API

---

## ğŸ“ Project Structure

```
src/ai_memory_layer/
â”œâ”€â”€ main.py              # FastAPI app entry point
â”œâ”€â”€ config.py            # Settings (database, API keys, etc.)
â”œâ”€â”€ database.py          # Database connection setup
â”‚
â”œâ”€â”€ routes/              # API ENDPOINTS (HTTP layer)
â”‚   â”œâ”€â”€ messages.py      # POST /v1/messages (store messages)
â”‚   â”œâ”€â”€ memory.py        # GET /v1/memory/search (search memories)
â”‚   â””â”€â”€ admin.py         # Admin endpoints (health, retention)
â”‚
â”œâ”€â”€ schemas/             # DATA VALIDATION (request/response models)
â”‚   â”œâ”€â”€ messages.py
â”‚   â”œâ”€â”€ memory.py
â”‚   â””â”€â”€ admin.py
â”‚
â”œâ”€â”€ services/            # BUSINESS LOGIC
â”‚   â”œâ”€â”€ message_service.py   # Main orchestrator (ingest/retrieve)
â”‚   â”œâ”€â”€ embedding.py         # Convert text â†’ vector
â”‚   â”œâ”€â”€ importance.py        # Calculate message importance (0-1)
â”‚   â”œâ”€â”€ retrieval.py         # Rank memories (similarity + importance)
â”‚   â””â”€â”€ retention.py         # Archive/delete old messages
â”‚
â”œâ”€â”€ repositories/        # DATABASE ACCESS (SQL queries)
â”‚   â””â”€â”€ memory_repository.py
â”‚
â””â”€â”€ models/              # DATABASE TABLES (ORM models)
    â””â”€â”€ memory.py        # Message, ArchivedMessage, RetentionPolicy tables

tests/                   # TESTS
â”œâ”€â”€ unit/                # Test individual functions
â”œâ”€â”€ integration/         # Test API endpoints
â””â”€â”€ e2e/                 # Test full workflows
```

---

## ğŸ”„ How It Works (Simple Flow)

### 1. Store a Message
```
User sends: POST /v1/messages
  â†’ Route validates request
  â†’ Service creates message in DB
  â†’ Service calculates importance score
  â†’ Service generates embedding (text â†’ vector)
  â†’ Service updates message with embedding
  â†’ Returns message ID
```

### 2. Search Memories
```
User sends: GET /v1/memory/search?query=Python
  â†’ Route validates query
  â†’ Service embeds query text â†’ vector
  â†’ Service fetches candidate messages from DB
  â†’ Service ranks them:
     - Similarity: How similar to query? (60%)
     - Importance: How important? (30%)
     - Decay: How recent? (10%)
  â†’ Returns top 5 most relevant messages
```

### 3. Archive Old Messages
```
Admin sends: POST /v1/admin/retention/run
  â†’ Service finds old/low-importance messages
  â†’ Service moves them to archive table
  â†’ Service deletes very old archived messages
  â†’ Returns counts
```

---

## âœ… What's Implemented

### Core Features
- âœ… **Message Storage**: Store messages with metadata
- âœ… **Embeddings**: Convert text to vectors (mock provider)
- âœ… **Importance Scoring**: Calculate importance (recency + role + explicit)
- âœ… **Memory Search**: Find relevant memories using similarity + importance
- âœ… **Retention**: Archive and delete old messages
- âœ… **Health Check**: `/v1/admin/health` endpoint
- âœ… **Metrics**: Prometheus metrics at `/metrics`

### Infrastructure
- âœ… **FastAPI**: REST API framework
- âœ… **Database Models**: SQLAlchemy ORM models defined
- âœ… **Migrations**: Alembic setup with initial migration
- âœ… **Docker**: Dockerfile + docker-compose.yml
- âœ… **Logging**: Structured logging with structlog
- âœ… **Security**: API key authentication (optional)
- âœ… **Tests**: Unit, integration, and E2E tests

### Code Quality
- âœ… **Type Hints**: Full type coverage
- âœ… **Linting**: Ruff + mypy configured
- âœ… **Async**: All I/O is async
- âœ… **Clean Architecture**: Layered design (routes â†’ services â†’ repositories)

---

## âŒ What's Remaining

### Critical (Must Have)
1. **Real Embedding Provider**
   - Currently uses mock (hash-based)
   - Need: OpenAI or Azure OpenAI integration
   - File: `services/embedding.py`

2. **Production Database Setup**
   - Alembic migrations exist but need to run
   - Need: Connect to real Postgres with pgvector
   - Command: `alembic upgrade head`

3. **Background Jobs**
   - Embedding generation is synchronous (slow)
   - Need: Celery/Arq for async embedding jobs
   - File: `models/memory.py` has `EmbeddingJob` table (unused)

4. **Scheduled Retention**
   - Retention must be triggered manually
   - Need: Cron job or scheduler to run automatically
   - Can use: Celery beat, Kubernetes CronJob, or external scheduler

### Important (Should Have)
5. **More Tests**
   - Current: Basic tests exist
   - Need: Higher coverage, edge cases, load tests

6. **Error Handling**
   - Basic error handling exists
   - Need: Better error messages, retry logic, circuit breakers

7. **Documentation**
   - README exists
   - Need: API examples, deployment guide, architecture docs

8. **Monitoring**
   - Metrics exist
   - Need: Alerting, dashboards, tracing

### Nice to Have
9. **Multi-tenancy**
   - Basic tenant_id support exists
   - Need: Tenant isolation, rate limiting per tenant

10. **Caching**
    - No caching currently
    - Need: Redis for frequent queries

---

## ğŸ§ª How to Test Everything

### 1. Setup Environment

```bash
# Install dependencies
uv sync

# Or with pip
pip install -e .[dev]
```

### 2. Setup Database

**Option A: Docker Compose (Recommended)**
```bash
# Start Postgres + API
docker compose up -d

# Run migrations
docker compose exec api alembic upgrade head
```

**Option B: Local Postgres**
```bash
# Create database
createdb memory_layer

# Set environment
export MEMORY_DATABASE_URL="postgresql+asyncpg://user:pass@localhost/memory_layer"

# Run migrations
alembic upgrade head
```

**Option C: SQLite (Testing Only)**
```bash
# Uses default SQLite (no setup needed)
# Just run: make test
```

### 3. Run Tests

```bash
# Run all tests
make test
# or
pytest

# Run with coverage
pytest --cov=src/ai_memory_layer --cov-report=html

# Run specific test types
pytest tests/unit/              # Unit tests
pytest tests/integration/       # Integration tests
pytest tests/e2e/               # End-to-end tests

# Run with verbose output
pytest -v
```

### 4. Test API Manually

```bash
# Start API
make run
# or
uvicorn ai_memory_layer.main:app --reload

# Visit API docs
open http://localhost:8000/docs

# Test endpoints:
# 1. Store message
curl -X POST http://localhost:8000/v1/messages \
  -H "Content-Type: application/json" \
  -d '{
    "tenant_id": "test",
    "conversation_id": "conv-1",
    "role": "user",
    "content": "I love Python programming"
  }'

# 2. Search memories
curl "http://localhost:8000/v1/memory/search?tenant_id=test&query=Python&top_k=5"

# 3. Health check
curl http://localhost:8000/v1/admin/health

# 4. Metrics
curl http://localhost:8000/metrics
```

### 5. Test with Docker

```bash
# Build and run
docker compose up --build

# Test API
curl http://localhost:8000/v1/admin/health

# View logs
docker compose logs -f api

# Stop
docker compose down
```

### 6. Pre-Deployment Checklist

```bash
# âœ… Code quality
make lint              # Check for errors
make format            # Auto-format code

# âœ… Tests pass
make test              # All tests green

# âœ… Database migrations
alembic upgrade head   # Migrations applied

# âœ… Environment variables set
# Check .env file has:
# - MEMORY_DATABASE_URL
# - MEMORY_EMBEDDING_PROVIDER (if using real provider)
# - MEMORY_API_KEYS (if using auth)

# âœ… Health check works
curl http://localhost:8000/v1/admin/health

# âœ… Metrics work
curl http://localhost:8000/metrics
```

---

## ğŸš€ Deployment Steps

### 1. Prepare Environment
```bash
# Set production database URL
export MEMORY_DATABASE_URL="postgresql+asyncpg://user:pass@prod-db/memory_layer"

# Set embedding provider
export MEMORY_EMBEDDING_PROVIDER="openai"  # or "azure_openai"
export OPENAI_API_KEY="sk-..."  # if using OpenAI

# Set API keys (comma-separated)
export MEMORY_API_KEYS="key1,key2,key3"
```

### 2. Run Migrations
```bash
alembic upgrade head
```

### 3. Deploy
```bash
# Using Docker
docker build -t ai-memory-layer .
docker run -p 8000:8000 --env-file .env ai-memory-layer

# Or using Docker Compose
docker compose -f docker-compose.prod.yml up -d
```

### 4. Verify
```bash
# Health check
curl http://your-server:8000/v1/admin/health

# Test endpoint
curl -X POST http://your-server:8000/v1/messages \
  -H "x-api-key: your-key" \
  -H "Content-Type: application/json" \
  -d '{...}'
```

---

## ğŸ“Š Current Status

**Completion: ~75%**

- âœ… Core functionality: **100%**
- âœ… Infrastructure: **90%**
- âš ï¸ Production readiness: **60%**
- âš ï¸ Testing: **70%**
- âŒ Background jobs: **0%**
- âŒ Real embeddings: **0%**

**Next Priority**: Implement real embedding provider (OpenAI/Azure) and background job system.

---

## ğŸ”— Key Files to Know

- `main.py` - Start here, app entry point
- `routes/messages.py` - API endpoints
- `services/message_service.py` - Main business logic
- `repositories/memory_repository.py` - Database queries
- `config.py` - All configuration
- `docker-compose.yml` - Local development setup

---

## ğŸ’¡ Quick Commands Reference

```bash
# Development
make run              # Start API
make test             # Run tests
make lint             # Check code
make format           # Format code

# Database
alembic upgrade head  # Apply migrations
alembic revision --autogenerate -m "description"  # Create migration

# Docker
docker compose up     # Start services
docker compose down   # Stop services
docker compose logs   # View logs
```

